# 2026ë…„ 1ì›” ì…‹ì§¸ì£¼ íšŒê³ 

# 2026.01.12

STT ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ë§ì€ ì‹œí–‰ì°©ì˜¤ë¥¼ ê±°ì³¤ë‹¤. íŠ¹íˆ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ë˜ëŠ” ì„œë¹„ìŠ¤ì—ì„œì˜ AI ë³€í˜•ì´ê¸° ë•Œë¬¸ì—, ë¹ ë¥¸ aië¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì„ ì ‘í–ˆë‹¤.

ì‚¬ìš©í•œ ëª¨ë¸ì€ OpenAIì˜ Whisper ëª¨ë¸ë¡œ, ê°™ì€ ëª¨ë¸ì´ì§€ë§Œ ë‹¤ì–‘í•œ ë²„ì „ì´ ì¡´ì¬í–ˆë‹¤.

```python
import asyncio
import numpy as np
import av
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from aiortc import RTCPeerConnection, RTCSessionDescription
from faster_whisper import WhisperModel

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

pcs = set()

model = WhisperModel("turbo", device="cuda", compute_type="float16")

class AudioProcessor:
    def __init__(self, track, get_channel_func):
        self.track = track
        self.get_channel = get_channel_func
        self.audio_buffer = np.array([], dtype=np.float32)
        self.resampler = av.AudioResampler(
            format="s16",
            layout="mono",
            rate=16000
        )

    async def start(self):
        try:
            while True:
                frame = await self.track.recv()
                resampled_frames = self.resampler.resample(frame)

                for f in resampled_frames:
                    array = (
                        f.to_ndarray()
                        .flatten()
                        .astype(np.float32) / 32768.0
                    )
                    self.audio_buffer = np.concatenate(
                        [self.audio_buffer, array]
                    )

                if len(self.audio_buffer) >= int(16000 * 1.5):
                    audio_segment = self.audio_buffer.copy()
                    self.audio_buffer = np.array([], dtype=np.float32)

                    loop = asyncio.get_running_loop()
                    segments, _ = await loop.run_in_executor(
                        None,
                        lambda: model.transcribe(
                            audio_segment,
                            language="ko"
                        )
                    )

                    result_text = "".join(
                        seg.text for seg in segments
                    ).strip()

                    if result_text:
                        channel = self.get_channel()
                        if channel and channel.readyState == "open":
                            channel.send(result_text)

        except Exception as e:
            print(e)

@app.post("/offer")
async def offer(params: dict):
    pc = RTCPeerConnection()
    pcs.add(pc)

    state = {"channel": None}

    @pc.on("datachannel")
    def on_datachannel(channel):
        state["channel"] = channel

    @pc.on("track")
    def on_track(track):
        if track.kind == "audio":
            processor = AudioProcessor(
                track,
                lambda: state["channel"]
            )
            asyncio.create_task(processor.start())

    @pc.on("connectionstatechange")
    async def on_connectionstatechange():
        if pc.connectionState in ["failed", "closed"]:
            await pc.close()
            pcs.discard(pc)

    await pc.setRemoteDescription(
        RTCSessionDescription(
            sdp=params["sdp"],
            type=params["type"]
        )
    )

    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    while pc.iceGatheringState != "complete":
        await asyncio.sleep(0.1)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

```

ì‹¤ì œë¡œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•  ë•Œ ì“´ ì½”ë“œë¡œ, ëª¨ë¸ ëª…ì„ ë°”ê¾¸ë©´ì„œ SSAFY ì™€ì´íŒŒì´ë¥¼ ê±°ì˜ ë…ì í•˜ë“¯ì´ ëª¨ë¸ë“¤ì„ ë§ˆêµ¬ë§ˆêµ¬ ë‹¤ìš´ ë°›ì•˜ë‹¤. (ì£„ì†¡í•©ë‹ˆë‹¤ ã…ã…)

whisperëŠ” 68ë§Œ ì‹œê°„ ë¶„ëŸ‰ì˜ ë‹¤êµ­ì–´ ë° ë‹¤ëª©ì  ê°ë… ë°ì´í„°ë¥¼ í•™ìŠµí–ˆìœ¼ë©°, ìœ íŠœë¸Œ ì˜ìƒì„ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµí–ˆë‹¤ê³  í•œë‹¤.

ì²˜ìŒì—ëŠ” python ì„œë²„ë¥¼ ë§Œë“¤ê³ , ì„ì‹œë¡œ Vueë¡œ í”„ë¡ íŠ¸ë¥¼ ë§Œë“¤ì–´ í†µì‹ í•˜ë©° ì†ë„ë¥¼ í™•ì¸í–ˆë‹¤.

| ëª¨ë¸ | ì •í™•ë„ | í‰ê·  ì§€ì—° ì†ë„ (ì´ˆ) | í™˜ê° |
| --- | --- | --- | --- |
| tiny | 0.64 | 0.8 | ì ìŒ |
| base | 0.71 | 1.1 | ì ìŒ |
| small | 0.70 | 1.4 | ì ìŒ |
| medium | 0.68 | 2.2 | ë³´í†µ |
| large | 0.68 | 3.3 | ì‹¬í•¨ |
| turbo | 0.65 | 3.4 | ì‹¬í•¨ |

ì •í™•ë„ëŠ” ëŒ€ë³¸ì„ ë¯¸ë¦¬ ì •í•´ë‘ê³  ê·¸ê²ƒì„ ë”°ë¼ ì½ìœ¼ë©° ì •í™•ë„ë¥¼ ë¶„ì„í–ˆë‹¤.
mediumì´ìƒìœ¼ë¡œëŠ” ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ì— ë”°ë¼ì„œ ì¶”ë¡  ì†ë„ê°€ ëŠ˜ì–´ë‚˜ê³ , í™˜ê°ì„ ë§ì´ ì¼ìœ¼ì¼°ë‹¤. 
ê·¸ë˜ì„œ ë‚˜ëŠ” ì •í™•ë„ì— ë¹„í•´ ì§€ì—°ì†ë„ê°€ ì§§ì€ baseì™€ smallì„ ì‚¬ìš©í•  ëª¨ë¸ë¡œ ì„ íƒí–ˆë‹¤.

í•˜ì§€ë§Œ ìš°ë¦¬ì¡°ëŠ” ì‹¤ì‹œê°„ ìë§‰ì„ í†µí•´ì„œ ë˜ ë‹¤ë¥¸ AIê°€ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— ì–´ëŠì •ë„ ë³´ì •ì´ í•„ìš”í–ˆë‹¤. ë¨¸ë¦¿ì†ì—ì„œ ë– ì˜¤ë¥¸ ë³´ì •ë²•ì€ í¬ê²Œ ë‘ê°€ì§€ê°€ ìˆì—ˆë‹¤.

1. ì¡ìŒ ì œê±°.
2. Whisper-Prompt ì—”ì§€ë‹ˆì–´ë§

ìš°ì„  ì¡ìŒì„ ì œê±°í•¨ìœ¼ë¡œì¨ Whisperê°€ ì£¼ë³€ì˜ ì†ŒìŒì„ ëª» ì¸ì‹í•˜ë„ë¡ í–ˆë‹¤. ì¡ìŒ ì œê±°ëŠ” ì¼ë¶€ ë°ì‹œë²¨ì„ ì¤„ì´ê±°ë‚˜ í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì‹œë„í–ˆìœ¼ë‚˜, AI ì„œë²„ë‹¨ì—ì„œ ì‹œí–‰í•  ê²½ìš° ì‹œê°„ì´ ìƒê°ë³´ë‹¤ ë” ì¡ì•„ë¨¹ì–´ì„œ ì¶”í›„ WebRTC ê¸°ìˆ ì„ êµ¬í˜„í•¨ê³¼ ë™ì‹œì— í”„ë¡ íŠ¸ì—ì„œ ì²˜ë¦¬í•˜ê¸°ë¡œ í–ˆë‹¤.

ë‘ ë²ˆì§¸ë¡œëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë‹¤. WhisperëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì¤„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ ì¶”ë¡ ì— íŒíŠ¸ë¥¼ ë„£ì–´ ì¤„ ìˆ˜ ìˆë‹¤. ì›”ìš”ì¼ ë‹¹ì‹œì—ëŠ” í˜ë¥´ì†Œë‚˜ë¥¼ ëª…í™•íˆ ì •í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì—ˆë‹¤.

â€ë“¤ì–´ì˜¤ëŠ” ìŒì„±ì€ ìƒë‹´ì„ ë°›ìœ¼ë ¤ê³  í•˜ëŠ” ì‚¬ëŒì´ë‹¤.â€

â€œí•œêµ­ì–´ë¥¼ ì‚¬ìš©í•œë‹¤.â€

â€œìƒí’ˆì˜ ê³ ì¥ì— ëŒ€í•´ ìƒë‹´ì„ ë°›ìœ¼ë ¤ê³  í•œë‹¤.â€

í”„ë¡¬í”„íŠ¸ë¥¼ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆˆ ì´ìœ ëŠ”, í•˜ë‚˜ì˜ ë¬¸ì¥ì— ë„£ê²Œ ëœë‹¤ë©´ AIëŠ” í˜¼ë€ì„ ì¼ìœ¼í‚¨ë‹¤. AND ì¸ì§€, ORì¸ì§€, í•˜ëŠ” ê³ ë¯¼ì„ í•¨ê³¼ ë™ì‹œì— ì¶”ë¡  ëŠ¥ë ¥ì— ì´ìƒì´ ìƒê¸´ë‹¤. ë”°ë¼ì„œ ë¬¸ì¥ì„ ì„¸ê°€ì§€ë¡œ ë¶„í•  í•œë’¤ì— í•œë²ˆì— ë„£ì–´ì£¼ì—ˆë‹¤.

ì¡ìŒ ì œê±°ë¥¼ í•˜ê³  ë‚œ ë’¤, í™˜ê° í˜„ìƒì´ í™•ì‹¤íˆ ì¤„ì–´ ë“¤ì—ˆë‹¤. ê·¸ë¦¬ê³  í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­ë˜ëŠ” í˜„ìƒì´ ë§ì´ ì¤„ì—ˆë‹¤. ì›”ìš”ì¼ì€ ì´ í›„ íšŒê³ ì™€ ë™ì‹œì— ê³µë¶€ë¥¼ ëëƒˆë‹¤.

# 2026.01.13

í™”ìš”ì¼ì€ ìŒì„±ì—ì„œ ì‚¬ìš©ìì˜ ê°ì •ì„ ì½ì„ ìˆ˜ ì—†ì„ê¹Œ í•˜ëŠ” ê³µë¶€ì™€ í•¨ê»˜ ì‹œì‘í–ˆë‹¤. ì‚¬ìš©ìì˜ ê°ì •ì„ íŒë³„í•˜ë©´ STTê°€ ì¡°ê¸ˆë” ì¶”ë¡ ì„ ì‰½ê²Œ í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ í•˜ëŠ” ì´ìœ ì˜€ë‹¤. 

ì›”ìš”ì¼ê³¼ ë˜‘ê°™ì´ ì—¬ëŸ¬ê°€ì§€ ëª¨ë¸ì— ëŒ€í•´ ì¡°ì‚¬í•˜ë©´ì„œ ë‹¤ì‹œ ì™€ì´íŒŒì´ë¥¼ ë…ì í•˜ê¸°ë¡œ í–ˆë‹¤.
ì‚¬ì‹¤ ëª¨ë¸ë“¤ì€ ë§ì´ ë‹¤ìš´ë¡œë“œ ë°›ì§€ ì•Šì•˜ì§€ë§Œ, ê°€ìƒí™˜ê²½ì„ ê¸°ì¡´ì— ë§ˆêµ¬ì¡ì´ë¡œ í•´ì§‘ì€ ê²°ê³¼ ë”ì°í•œ ì•…ëª½ì˜ ìœµí•©ì²´ê°€ ë˜ì–´ ì˜ì¡´ì„±ë¼ë¦¬ ì¶©ëŒí•˜ëŠ” ì¼ì´ ë°œìƒí•˜ì—¬ ê°€ìƒí™˜ê²½ì„ ì‹¹ ë‚ ë ¤ë²„ë¦¬ê³ , ëª¨ë¸ì— ë§ëŠ” ì˜ì¡´ì„±ê³¼ íŒ¨í‚¤ì§€ë“¤ì„ ë°›ì•„ì„œ ì ìš©í–ˆë‹¤.  (ì£„ì†¡í•©ë‹ˆë‹¤ ã…ã…)

ì²˜ìŒ ì¨ë³¸ ëª¨ë¸ì€ E2V , Emotion 2 Voice ë¼ëŠ” ëª¨ë¸ì´ì˜€ë‹¤. ì²˜ìŒ ëª¨ë¸ì„ ë°›ì„ë•Œ ë§ì€ ì‹œí–‰ì°©ì˜¤ê°€ ìˆì—ˆë‹¤. í—ˆê¹…í˜ì´ìŠ¤ì—ì„œì˜ ëª¨ë¸ì˜ ì£¼ì†Œê°€ ë°”ë€ŒëŠ” ë°”ëŒì— ê³„ì† ì„¤ì¹˜ë¥¼ í•˜ì§€ ëª»í•´ ì§ì ‘ ì°¾ì•„ ë‚˜ì„°ë‹¤. ê¸ˆë°© ì°¾ì„ ìˆ˜ ìˆì—ˆë‹¤. ëª¨ë¸ì„ ì„¤ì¹˜í•˜ê³  ì„ì‹œ íŒŒì¼ì„ ë§Œë“¤ì–´ ì‹¤í—˜í–ˆë‹¤.

```python
from funasr import AutoModel
import os

model_id = "iic/emotion2vec_plus_large"

model = AutoModel(
    model=model_id,
    hub="hf",
)

wav_file = "sample.wav"

rec_result = model.generate(
    input=wav_file,
    output_dir="./outputs",
    granularity="utterance",
    extract_embedding=True
)

for result in rec_result:
    scores = result["scores"]
    labels = result["labels"]

    max_idx = scores.index(max(scores))

    print(f"ìµœì¢… ì˜ˆì¸¡ ê°ì •: {labels[max_idx]}")
    print(f"í™•ì‹ ë„: {scores[max_idx] * 100:.2f}%")

```

ì´ë²ˆì—ëŠ” AIì„œë²„ë¡œ ë§Œë“¤ì§€ ì•Šì€ ì´ìœ ê°€ ìˆë‹¤. RTCë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë¹ ë¥¸ í†µì‹  ë•Œë¬¸ì¸ë°, ì¤‘ê°„ì—ì„œ ì¸ê³µì§€ëŠ¥ ì„œë²„ê°€ ê°€ë¡œì±„ë©´ RTCë¥¼ ì‚¬ìš©í•  ì´ìœ ê°€ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ AIëŠ” ìƒë‹´ì‚¬ ì¸¡ì˜ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë³„ê°œë¡œ ë¡œì»¬ì—ì„œ í”„ë¡œê·¸ë¨ì„ ì˜¬ë ¤ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆë‹¤. 

AutoModel ì€ FunASRì—ì„œ ì œê³µí•˜ëŠ” AI ëª¨ë¸ ë¡œë”ë¡œ, ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì§€ì› í•˜ê¸°ì— ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆë‹¤.

E2Vë¥¼ ì—¬ëŸ¬ë²ˆ ì‚¬ìš©í–ˆì§€ë§Œ, ê²°ê³¼ëŠ” ì¢‹ì§€ ì•Šì•˜ë‹¤. ì£¼ë¡œ ì™¸êµ­ì¸ë“¤ì˜ ê°ì •ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì—, í•œêµ­ì¸ì˜ ê°ì • ë¶„ì„ì— ëŒ€í•œ ê²°ê³¼ëŠ” ì¢‹ì§€ ëª»í•œ ì„±ì ì´ì˜€ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ê°€ ì§ì ‘ ëª¨ë¸ì„ ë§Œë“¤ì–´ì•¼ í•  í•„ìš”ê°€ ìˆì—ˆë‹¤.

ê·¸ë˜ì„œ ìš°ì„  ëŒ€í•™ì›ìƒ ë•Œ ì“°ë˜ LSTM-AEë¥¼ ë‹¤ì‹œ êº¼ë‚´ë³´ê²Œ ë˜ì—ˆë‹¤.

```python
import torch
import torch.nn as nn

class LSTMAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.LSTM(10, 32, batch_first=True)
        self.decoder = nn.LSTM(32, 10, batch_first=True)

    def forward(self, x):
        enc, _ = self.encoder(x)
        dec, _ = self.decoder(enc)
        return dec

model = LSTMAE()
x = torch.randn(8, 50, 10)
y = model(x)

print("Output shape:", y.shape)

```

LSTM-AEë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”, ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì••ì¶•í–ˆë‹¤ ë³µì›í•˜ëŠ” ì‹ ê²½ë§ìœ¼ë¡œ, ì‹œê°„ì  íë¦„ì„ ê°€ì§„ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê¸°ì— ì í•©í•˜ë‹¤ê³  ìƒê°í–ˆê¸° ë•Œë¬¸ì´ë‹¤.

í•˜ì§€ë§Œ, ìš°ë¦¬í•œí…Œ ì£¼ì–´ì§„ 3ì£¼ë‚´ì— ëª¨ë¸ì„ í•™ìŠµí•œë‹¤ëŠ” ë³´ì¥ì´ ì—†ì–´ ìš°ì„  í…ŒìŠ¤íŠ¸ë§Œ í•˜ê³  í™”ìš”ì¼ì„ ë§ˆì³¤ë‹¤.

# 2026.01.14

ìŒì„±ì„ í†µí•œ AIì— ëŒ€í•´ ê³„ì† ìƒê°í•˜ë˜ ë„ì¤‘, ìš°ë¦¬ì˜ ì„œë¹„ìŠ¤ëŠ” ëª¨ë°”ì¼ë¡œ í†µí™”í•˜ëŠ” ê²ƒì„ì„ ê¹¨ë‹«ê³  ì„ì‹œë¡œ ë””ìŠ¤ì½”ë“œë¼ëŠ” í”„ë¡œê·¸ë¨ì„ í†µí•´ ì›¹ê³¼ PCê°„ í†µì‹ ì„ êµ¬í˜„í•˜ì˜€ë‹¤.

ë””ìŠ¤ì½”ë“œì˜ ì¶œë ¥ì„ vbaudiocable ë¼ëŠ” í”„ë¡œê·¸ë¨ì„ í†µí•´ ê°€ìƒì˜ í˜¸ìŠ¤ë¡œ í¬ë¡¬ ì…ë ¥ ì¥ì¹˜ë¡œ ë³´ë‚´ íœ´ëŒ€í°ìœ¼ë¡œ í†µì‹ í•˜ëŠ” ë“¯í•œ ëŠë‚Œì„ ì£¼ì—ˆë‹¤.

 

ë””ìŠ¤ì½”ë“œì˜ ì¡ìŒ ì œê±° ì„±ëŠ¥ì´ ë¬´ì²™ ë›°ì–´ë‚˜, ì •í™•ë„ê°€ ë§ì´ ì˜¬ë¼ê°”ë‹¤. ë””ìŠ¤ì½”ë“œë¥¼ í†µí•´ ë‚´ ìŒì„±ì„ íœ´ëŒ€í°ì—ì„œ ì»´í“¨í„°ë¡œ ì „ì†¡í•´ ì£¼ì—ˆë‹¤.

1.5ì´ˆ ë‹¨ìœ„ë¡œ ì‚¬ìš´ë“œë¥¼ ì €ì¥í•´ STTë¡œ ë³€í™˜í•˜ì˜€ì§€ë§Œ, ì¤‘ê°„ì— ê³„ì† ëŠê¸°ëŠ” ì¼ì´ ìˆì–´ ìƒˆë¡œìš´ ë°©ë²•ì„ ì‹œë„í•˜ê¸°ë¡œ í–ˆë‹¤.

0.3 ì´ˆ ë™ì•ˆ ìŒì„±ì´ ë“¤ë¦¬ì§€ ì•Šìœ¼ë©´ ìŒì„±ì„ ëŠì–´ AI ë³€í™˜ì„ ì‹œì‘í–ˆë‹¤.

ì´í›„ Bí˜• ê³µë¶€ë¥¼ í•˜ëŸ¬ê°”ë‹¤.

# 2026.01.15

ëª©ìš”ì¼ì€ ë‹¤ì–‘í•œ ê²ƒë“¤ì„ ì‹œë„í–ˆë‹¤. ìš°ì„  í…ìŠ¤íŠ¸ì˜ ê³µê²©ì„±ì„ ë¶„ì„í•˜ëŠ” AI ëª¨ë¸ì„ í™œìš©í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ê°€ì§€ ì°¾ì•„ë´¤ë‹¤.

ì²˜ìŒ ì°¾ì•„ë³¸ê±´ ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ì˜ unsmile-aië¡œ ê³µê²©ì„±ì„ ë¶„ì„í•˜ëŠ” aiëª¨ë¸ì´ì˜€ë‹¤. í•˜ì§€ë§Œ ì´ˆê¸°ì— cuda ì„¤ì •ìœ¼ë¡œ ì¸í•´ ëª¨ë¸ ì‹¤í–‰ì´ ì•ˆë˜ì–´ ìš°ì„  ë’¤ë¡œ ë¯¸ë¤„ë’€ë‹¤.

ê·¸ ë‹¤ìŒì€ beomi/KcELECTRA-base-v2022 ëª¨ë¸ì´ë‹¤. í•´ë‹¹ ëª¨ë¸ì€ í˜ì˜¤ í‘œí˜„ ë¶„ë¥˜ê¸°ë‹¤.

í•˜ì§€ë§Œ í˜ì˜¤ í‘œí˜„ ë¶„ë¥˜ê¸°ëŠ” ë‹¨ìˆœí•œ ë¹„ê¼¼ë“±ì„ ì˜ ì¸ì§€ í•˜ì§€ ëª»í•´ì„œ beomiì˜ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ì¸ kcbert-baseë¥¼ ì‚¬ìš©í–ˆë‹¤.

í•˜ì§€ë§Œ ì´ ì—­ì‹œ ì˜¬ë°”ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•Šì•„ ê²°êµ­ unsmileì„ ìœ„í•´ í™˜ê²½ì„ ìƒˆë¡œ ë’¤ì—ì—ˆë‹¤.

```python
import torch
import gradio as gr
from transformers import pipeline

USE_GPU = torch.cuda.is_available()
DEVICE = 0 if USE_GPU else -1

MODEL_NAME = "smilegate-ai/kor_unsmile"

try:
    toxicity_clf = pipeline(
        "text-classification",
        model=MODEL_NAME,
        device=DEVICE,
        top_k=None
    )
    model_loaded = True
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    toxicity_clf = None
    model_loaded = False

def analyze_toxicity(text):
    if not text or not text.strip():
        return 0.0, "ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.", ""

    if not model_loaded:
        return 0.0, "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨", "ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        results = toxicity_clf(text)[0]

        if isinstance(results, list):
            hate_score = 0.0
            hate_labels = []

            for item in results:
                label = item["label"].lower()
                score = float(item["score"])

                if any(keyword in label for keyword in ["hate", "ìš•ì„¤", "í˜ì˜¤", "ê³µê²©", "ì•…í”Œ", "clean"]):
                    if "clean" not in label and "none" not in label:
                        if score > hate_score:
                            hate_score = score
                        hate_labels.append(f"{item['label']}: {score:.3f}")

            all_labels = "\n".join(
                [f"{item['label']}: {item['score']:.3f}" for item in results[:5]]
            )

            if hate_score > 0.5:
                return hate_score, "âš ï¸ ê³µê²©ì  í‘œí˜„ ê°ì§€", all_labels
            else:
                return hate_score, "âœ… ì •ìƒì ì¸ í‘œí˜„", all_labels

        else:
            label = results["label"].lower()
            score = float(results["score"])

            if any(keyword in label for keyword in ["hate", "ìš•ì„¤", "í˜ì˜¤", "ê³µê²©", "ì•…í”Œ"]):
                return score, "âš ï¸ ê³µê²©ì  í‘œí˜„ ê°ì§€", f"{results['label']}: {score:.3f}"
            else:
                toxicity_score = 1.0 - score
                if toxicity_score > 0.5:
                    return toxicity_score, "âš ï¸ ê³µê²©ì  í‘œí˜„ ê°ì§€", f"{results['label']}: {score:.3f}"
                else:
                    return toxicity_score, "âœ… ì •ìƒì ì¸ í‘œí˜„", f"{results['label']}: {score:.3f}"

    except Exception as e:
        return 0.0, f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}", ""

with gr.Blocks(theme=gr.themes.Ocean()) as demo:
    gr.Markdown("""
    ## ğŸ§ª í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê³µê²©ì„± ë¶„ì„ê¸°

    í˜„ì¬ ì‚¬ìš© ëª¨ë¸: **smilegate-ai/kor_unsmile**
    """)

    with gr.Row():
        with gr.Column():
            text_input = gr.Textbox(
                lines=5,
                label="í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ì…ë ¥",
                placeholder="ì˜ˆì‹œ:\n- ë„ˆ ì§„ì§œ ì™œ ì´ë ‡ê²Œ ë©ì²­í•˜ëƒ?\n- ì˜¤ëŠ˜ ë‚ ì”¨ ì •ë§ ì¢‹ë„¤ìš”!\n- ì´ ì˜í™” ì™„ì „ ì“°ë ˆê¸°ì•¼",
            )

            gr.Examples(
                examples=[
                    "ë„ˆ ì§„ì§œ ì™œ ì´ë ‡ê²Œ ë©ì²­í•˜ëƒ?",
                    "ì˜¤ëŠ˜ ë‚ ì”¨ ì •ë§ ì¢‹ë„¤ìš”!",
                    "êº¼ì ¸, ë„ˆ ê°™ì€ ì¸ê°„ì€ ì‹«ì–´",
                    "ì´ ì œí’ˆ í’ˆì§ˆì´ ì •ë§ ìš°ìˆ˜í•©ë‹ˆë‹¤",
                    "ë„ˆí¬ ê°™ì€ ì• ë“¤ì€ ì‚¬íšŒì— í•„ìš”ì—†ì–´",
                ],
                inputs=text_input,
            )

        with gr.Column():
            score_out = gr.Number(label="ğŸ“Š ê³µê²©ì„± ì ìˆ˜ (0~1)", precision=4)
            label_out = gr.Textbox(label="ğŸ· íŒì • ê²°ê³¼")
            detail_out = gr.Textbox(label="ğŸ“ ìƒì„¸ ë¶„ì„", lines=5)

    btn = gr.Button("ğŸ” ë¶„ì„ ì‹œì‘", variant="primary")

    btn.click(
        analyze_toxicity,
        inputs=text_input,
        outputs=[score_out, label_out, detail_out],
    )

if __name__ == "__main__":
    demo.launch()

```

ì¤‘ê°„ ì¤‘ê°„ ê³„ì† ì‹¤í–‰ì´ ì•ˆë˜ì„œ ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ë¥¼ ë§ì´ ì°ì—ˆë‹¤. AIì˜ í™˜ê²½ì€ ì •ë§ ê¹Œë‹¤ë¡œì›Œ ë³„ë„ì˜ ì»¨í…Œì´ë„ˆ ë¶„ë¦¬ê°€ í•„ìš”í•˜ë‹¤ ëŠê¼‡ë‹¤.

ì´ë²ˆì£¼ëŠ” ëª…ì„¸ì„œ ì‘ì„±ê³¼ í•¨ê»˜ ì‚¬ìš©í•  AIì˜ ê¸°ì´ˆë¥¼ ë‹¤ ì‘ì„±í–ˆë‹¤.

ê¸ˆìš”ì¼ì€ WebRTC êµ¬í˜„ì„ íŒ€ì›ê³¼ ê°™ì´ êµ¬í˜„í•´ë³¼ ì˜ˆì •ì´ë‹¤.